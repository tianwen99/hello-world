计算机科学/人工智能之父：艾伦.麦席森.图灵（计算机最高奖就是图灵奖）
提出了以二进制作为计算机的数制基础：冯.诺依曼

冯.诺依曼体系架构：五大部件
CPU：运算器、控制器、寄存器（临时，先从内存拿数据，CPU从寄存器拿数据运算完又存在寄存器，寄存器再到内存，寄存器很小很贵，频率与CPU一样）、多级缓存cache（指CPU的多级缓存，1级等于CPU频率，2级差一点，3级更慢，但都比内存快，最常用的数据可以放在缓存里，不用每次都去内存拿数据，1级2级是一个CPU独享，3级缓存是多个CPU共享）
I/O设备：输入设备、输出设备（硬盘是输入或输出设备，速度慢）
存储器：内存
CPU只跟内存打交道，再由内存跟I/O设备打交道

计算机语言：0和1指令（各家公司指代不一样，不统一）
汇编语言：用符号替代（各家公司的汇编不一样，每台CPU不一样）
低级语言：面向机器的语言（不同机器需要不同指令和汇编程序）
高级语言：接近自然语言和数学语言（Fortran是最早的高级语言）
语言越高级越接近人类的自然语言，语言越低级越能让机器理解
编译语言(C\C++)：把源代码转换成CPU指令
解释器(java\python\C#)：解释后转换成字节码，运行在虚拟机上，解释器执行中间代码（中间代码Bytecode)。


现在的高级语言：结构化语言（唯一入口和唯一出口）、面向对象语言（封装、继承、多态）、函数式语言（高阶函数）

程序=算法+数据结构
算法：是处理数据的方式，有优劣之分
数据结构：是数据在计算机中的类型和组织方式



Python解释器：
官方CPython：最广泛的Python解释器
IPython：交互式，功能增强的CPython
pypy
Jython：编译成java的字节码，跑在jvm上。
IronPython


0xa:16进制（a=10,b=11,..,f=15）
0o10:八进制
0b10:二进制（8421）

转义序列：
\t:tab键
\\:\
\r:回车
\n:换行
续行：在末尾用\,用括号


标识符：只能字母、下划线、数字，只能以字母或下划线开头，，不能用关键字，大小写敏感。
约定：不使用中文，不使用歧义单词，如class_,不要随便使用下划线开头的标识符。


常量：Python无法定义常量，有字面常量，如12。



Python是动态语言（不需要事先声明变量类型，可以改类型）、强类型语言（同类型才可以打印，不然会报错，要转换成同类型才能输出，print（ABC+str（123））


位运算：& | ~ ^ <<  >>
常用方式：乘除2的倍数，32//8=32//2**3，相当于32>>3(虚拟机会把它转换成32》3）
bin（12）：转换成2进制，'0b1100'=12(8*1+4*1+2*0+1*0=12）

~12=-13（在二进制里理解）：
原码：把它变成二进制，是什么样就是什么样，5=0b101,1=0b1,-1=-0b1（给人看）
反码：正数的反码与原码相同，负数的反码符号位不变，其余按位取反



补码：正数的补码与原码相同，负数的补码符号位不变（最高位不变），其余按位取反后+1（补码的补码就是原码，计算机看）
-1的原码：1000 0001
-1的补码：1111 1110+1=1111 1111

负数表示法：只有加法器，没有减法器，减法要转化成加法，负数在计算机使用补码存储
-1的补码：1111 1111
5-1=5+（-1）=0b101-0b1=0b101+0b11111111,溢出位舍弃
 5:0000 0101
-1:1111 1111
=  1111 1111
+          1
=0000 0000（够8位溢出后就是0）
+0000 0100
=0000 0100=4
先把5分成1+4，加1后溢出是0,0加上4就等于4
~12=-13（在二进制里理解）：
         12=0000 1100
~a:是按位取反：1111 0011
有最高位，计算机要转成原码给你看：1000 1101=-13

^:异或(异是1）
10^9=3
10=0000 1010
9 =0000 1001
  =0000 0011=3
10^-9=
-9原=1000 1001
-9补码=1111 0111
10    =0000 1010
      =1111 1101
转原=1000 0011=-3



a&b:按位乘
a|b：按位除


内存管理：
引用计数：当对象引用数变为0，它就被GC垃圾回收。
有关性能的时候，就需要考虑变量的引用问题，但是该释放内存还是尽量不释放内存，看需求。






















